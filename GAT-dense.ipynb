{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAT-dense.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"AeMWJ5A9P9wk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"a721a918-acc6-47ff-ad7f-724a38da0442","executionInfo":{"status":"ok","timestamp":1556202002794,"user_tz":-120,"elapsed":466,"user":{"displayName":"Lucas Kania","photoUrl":"https://lh3.googleusercontent.com/-Atm5piH2aes/AAAAAAAAAAI/AAAAAAAAjl0/ClTgWH2onX0/s64/photo.jpg","userId":"02510383906759004102"}}},"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"lG_WpMphP9wp","colab_type":"code","colab":{}},"cell_type":"code","source":["import os,sys,inspect\n","import os\n","import joblib\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import scipy.sparse.linalg as la\n","import scipy.sparse as sp\n","import scipy\n","import time\n","import pickle\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","%matplotlib inline\n","\n","import scipy.io as sio\n","import process_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rU-uChLjGaLO","colab_type":"code","outputId":"7ff4a53a-82d6-4a6b-af71-fd1037286bed","executionInfo":{"status":"ok","timestamp":1555840637854,"user_tz":-120,"elapsed":5960,"user":{"displayName":"Lucas Kania","photoUrl":"https://lh3.googleusercontent.com/-Atm5piH2aes/AAAAAAAAAAI/AAAAAAAAjl0/ClTgWH2onX0/s64/photo.jpg","userId":"02510383906759004102"}},"colab":{"base_uri":"https://localhost:8080/","height":267}},"cell_type":"code","source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-04-21 09:57:13--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.54.84.112, 52.201.75.180, 34.206.253.53, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.54.84.112|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14977695 (14M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  14.28M  8.40MB/s    in 1.7s    \n","\n","2019-04-21 09:57:15 (8.40 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14977695/14977695]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"metadata":{"id":"Ks7k1tKGP9ww","colab_type":"code","outputId":"867c423f-8f4c-4b9e-ce07-09250b3df711","executionInfo":{"status":"ok","timestamp":1556208030424,"user_tz":-120,"elapsed":301,"user":{"displayName":"Lucas Kania","photoUrl":"https://lh3.googleusercontent.com/-Atm5piH2aes/AAAAAAAAAAI/AAAAAAAAjl0/ClTgWH2onX0/s64/photo.jpg","userId":"02510383906759004102"}},"colab":{"base_uri":"https://localhost:8080/","height":1020}},"cell_type":"code","source":["LOG_DIR = './log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n","\n","\n","class GCN:\n","    \n","    \"\"\"\n","    The neural network model.\n","    \"\"\"\n","    \n","    def frobenius_norm(self, tensor):\n","        square_tensor = tf.square(tensor)\n","        tensor_sum = tf.reduce_sum(square_tensor)\n","        frobenius_norm = tf.sqrt(tensor_sum)\n","        return frobenius_norm\n","    \n","    def convert_coo_to_sparse_tensor(self, L):\n","        indices = np.column_stack((L.row, L.col))\n","        L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n","        L = tf.sparse_reorder(L)\n","        return L\n","      \n","    def concat(self,x,y):\n","      if x is None:\n","        return y\n","      if y is None:\n","        return x\n","      return tf.concat([x,y],axis=1)\n","    \n","    def get_variable(self,name,shape,reg=False):\n","        v = tf.get_variable(name,shape=shape,initializer=tf.contrib.layers.xavier_initializer())\n","        if reg:\n","          self.reg_variables.append(v)\n","        return v\n","      \n","    def get_regularized_variable(self,name,shape):\n","      return self.get_variable(name,shape,True)\n","    \n","    def get_bias(self,name,reg=False):\n","      v = tf.get_variable(name,[1], tf.float32,initializer=tf.constant_initializer(0.1))\n","      if reg:\n","          self.reg_variables.append(v)\n","      return v\n","    \n","    def get_regularized_bias(self,name):\n","      return self.get_bias(name,True)\n","    \n","    def __init__(self, A, X, Y,\n","                 num_hidden_feat,\n","                 K,\n","                 learning_rate,\n","                 gamma, # l2 regularization parameter\n","                 idx_gpu = '/gpu:2'):\n","        \n","        self.num_hidden_feat = num_hidden_feat\n","        self.learning_rate = learning_rate\n","        self.gamma=gamma\n","        self.num_layers=len(num_hidden_feat)\n","        \n","        self.K = K\n","        self.reg_variables = []\n","        with tf.Graph().as_default() as g:\n","                self.graph = g\n","                \n","                with tf.device(idx_gpu):\n","                        # dimensions\n","                        M = A.shape[0]\n","                  \n","                        #definition of constant matrices\n","                        self.A = tf.sparse.to_dense(self.convert_coo_to_sparse_tensor(A.tocoo()))\n","                        self.X = tf.constant(X, dtype=tf.float32) \n","                        self.Y = tf.constant(Y, dtype=tf.float32)\n","                        \n","                        #placeholder definition\n","                        self.idx_nodes = tf.placeholder(tf.int32)\n","                        self.keep_prob = tf.placeholder(tf.float32)\n","                        \n","                        #model definition\n","                        \n","                        M = self.A.get_shape().as_list()[0]\n","                        A_ = tf.add(self.A,tf.eye(M,M))\n","                        \n","                        H_prev = self.X\n","                        \n","                        for layer in range(self.num_layers):\n","                          \n","                          with tf.name_scope('layer_{0}'.format(layer)):\n","\n","                            Fout = self.num_hidden_feat[layer]\n","                            Fin = H_prev.get_shape().as_list()[1]\n","\n","                            H_prev = tf.nn.dropout(H_prev, self.keep_prob)\n","                            H = None\n","\n","                            for k in range(self.K[layer]):\n","\n","                              W = self.get_regularized_variable(\"W_{0}_{1}\".format(k,layer),shape=[Fin,Fout])\n","                              A_1 = self.get_regularized_variable(\"A_1_{0}_{1}\".format(k,layer),shape=[Fout,1])\n","                              A_2 = self.get_regularized_variable(\"A_2_{0}_{1}\".format(k,layer),shape=[Fout,1])\n","                              b_1 = self.get_regularized_bias(\"b_1_{0}_{1}\".format(k,layer))\n","                              b_2 = self.get_regularized_bias(\"b_2_{0}_{1}\".format(k,layer))\n","\n","                              U = tf.matmul(H_prev,W) # M x Fout\n","\n","                              gamma_1 = tf.matmul(U,A_1) + b_1 # M x 1\n","                              gamma_2 = tf.matmul(U,A_2) + b_2 # M x 1\n","\n","                              phi = tf.add(\n","                                  tf.multiply(A_,gamma_1),\n","                                  tf.multiply(A_,tf.transpose(gamma_2))) # M x M\n","\n","                              phi = tf.nn.leaky_relu(phi,alpha=0.2)\n","                              \n","                              psi = tf.exp(phi)\n","                              psi = tf.multiply(psi,A_)\n","                              \n","                              theta = tf.reduce_sum(psi,axis=-1)\n","                              \n","                              Q = tf.multiply(psi,tf.reciprocal(theta))\n","                           \n","                              Q = tf.nn.dropout(Q, self.keep_prob)\n","                              H_temp = tf.matmul(Q,U) # M x Fout                          \n","                              H = self.concat(H,H_temp)\n","\n","                            H_prev = H\n","\n","                            if layer != self.num_layers-1 : # it is not the last layer\n","                              H_prev = tf.nn.elu(H_prev)\n","                        \n","                        self.logits = H_prev\n","                        \n","                        self.l_out = tf.gather(self.logits, self.idx_nodes)\n","                        self.c_Y = tf.gather(self.Y, self.idx_nodes)\n","                        \n","                        #loss function definition\n","                        \n","                        with tf.name_scope('loss'):\n","                          self.l2_reg = 0\n","                          for W in self.reg_variables:\n","                            self.l2_reg += tf.nn.l2_loss(W)\n","\n","                          self.data_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.l_out, labels=self.c_Y)) \n","                          self.loss = self.data_loss + self.gamma*self.l2_reg\n","                        \n","                        #solver definition\n","                        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n","                        self.opt_step = self.optimizer.minimize(self.loss)\n","                        \n","                        #predictions and accuracy extraction\n","                        self.c_predictions = tf.argmax(tf.nn.softmax(self.l_out), 1)\n","                        self.accuracy = tf.contrib.metrics.accuracy(self.c_predictions, tf.argmax(self.c_Y, 1))\n","                        \n","                        #gradients computation\n","                        self.trainable_variables = tf.trainable_variables()\n","                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n","                        \n","                        #for i,t in enumerate(tf.trainable_variables()):\n","                        #  print(str(t) + \" \" + str(self.var_grad[i]))\n","                        \n","                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n","                        \n","                        #session creation\n","                        config = tf.ConfigProto(allow_soft_placement = True)\n","                        config.gpu_options.allow_growth = True\n","                        self.session = tf.Session(config=config)\n","                        \n","                        writer = tf.summary.FileWriter(LOG_DIR, self.session.graph)\n","\n","\n","                        #session initialization\n","                        init = tf.global_variables_initializer()\n","                        self.session.run(init)\n","                        \n","#learning parameters and path dataset\n","\n","num_total_iter_training = 3000\n","learning_rate = 0.005\n","val_test_interval = 1\n","num_hidden_feat = [8,7]\n","K = [8,1]\n","gamma = 0.0005\n","path_dataset = './CORA/dataset.pickle'\n","    \n","#dataset loadina\n","\n","A, X, Y, train_idx, val_idx, test_idx = process_data.load_data(\"cora\")\n","X = process_data.preprocess_features(X)\n","\n","# compute GCN adj matrix\n","A_tilde = sp.csr_matrix(A)\n","A_tilde.setdiag(1)\n","D = A_tilde.sum(axis=1)\n","\n","D_rows, D_cols = D.nonzero()\n","D_vals = [D[i,j] for i, j in zip(D_rows, D_cols)]\n","D_vals = np.reciprocal(np.sqrt(np.asarray(D_vals)))\n","\n","D_inv_sqrt = sp.csr_matrix((D_vals, (range(len(D_vals)), range(len(D_vals)))))\n","\n","A_tilde = D_inv_sqrt.dot(A_tilde).dot(D_inv_sqrt)\n","A_tilde = A_tilde.tocsr()\n","A_tilde.eliminate_zeros()\n","\n","# Training\n","\n","num_exp = 10 #number of times training GCN over the given dataset\n","\n","list_all_acc = []\n","list_all_cost_val_avg  = []\n","list_all_data_cost_val_avg = []\n","list_all_acc_val_avg   = []\n","list_all_cost_test_avg = []\n","list_all_acc_test_avg  = []\n","\n","num_done = 0\n","for seed in range(num_exp):\n","    GCNN = GCN(A=A_tilde, X=X, Y=Y, num_hidden_feat=num_hidden_feat, K=K, learning_rate=learning_rate, gamma=gamma)\n","\n","    cost_train_avg      = []\n","    grad_norm_train_avg = []\n","    acc_train_avg       = []\n","    cost_test_avg       = []\n","    grad_norm_test_avg  = []\n","    acc_test_avg        = []\n","    cost_val_avg        = []\n","    data_cost_val_avg   = []\n","    acc_val_avg         = []\n","    iter_test           = []\n","    list_training_time = list()\n","\n","    #Training code\n","    for i in range(num_total_iter_training):\n","        if (len(cost_train_avg) % val_test_interval) == 0:\n","\n","            #Validate the model\n","            tic = time.time()\n","            \n","            feed_dict = {GCNN.idx_nodes: val_idx, GCNN.keep_prob:1.0}\n","            acc_val, cost_val, data_cost_val = GCNN.session.run([GCNN.accuracy, GCNN.loss, GCNN.data_loss], feed_dict)\n","            \n","            data_cost_val_avg.append(data_cost_val)\n","            cost_val_avg.append(cost_val)\n","            acc_val_avg.append(acc_val)\n","\n","            #Test the model\n","            tic = time.time()\n","            \n","            feed_dict = {GCNN.idx_nodes: test_idx, GCNN.keep_prob:1.0}\n","            acc_test, cost_test = GCNN.session.run([GCNN.accuracy, GCNN.loss], feed_dict)\n","            \n","            cost_test_avg.append(cost_test)\n","            acc_test_avg.append(acc_test)\n","            iter_test.append(len(cost_train_avg))\n","\n","        tic = time.time()\n","\n","        tic = time.time()\n","        feed_dict = {GCNN.idx_nodes: train_idx, GCNN.keep_prob: 0.5}\n","        \n","        _, current_training_loss, norm_grad, current_acc_training = GCNN.session.run([GCNN.opt_step, GCNN.loss, GCNN.norm_grad, GCNN.accuracy], feed_dict) \n","\n","        training_time = time.time() - tic   \n","\n","        cost_train_avg.append(current_training_loss)\n","        grad_norm_train_avg.append(norm_grad)\n","        acc_train_avg.append(current_acc_training)\n","\n","    #Compute and print statistics of the last realized experiment\n","    list_all_acc.append(100*(np.asarray(acc_test_avg)[np.asarray(data_cost_val_avg)==np.min(data_cost_val_avg)]))\n","    list_all_cost_val_avg.append(cost_val_avg)\n","    list_all_data_cost_val_avg.append(data_cost_val_avg)\n","    list_all_acc_val_avg.append(acc_val_avg)\n","    list_all_cost_test_avg.append(cost_test_avg)\n","    list_all_acc_test_avg.append(acc_test_avg)\n","\n","    print('Num done: %d' % num_done)\n","    print('Max accuracy on test set achieved: %f%%' % np.max(np.asarray(acc_test_avg)*100))\n","    print('Max suggested accuracy: %f%%' % (100*(np.asarray(acc_test_avg)[np.asarray(data_cost_val_avg)==np.min(data_cost_val_avg)]),))\n","    print('Current mean: %f%%' % np.mean(list_all_acc))\n","    print('Current std: %f' % np.std(list_all_acc))\n","\n","    num_done += 1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["http://01c738b7.ngrok.io\n","(2708, 2708)\n","(2708, 1433)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py:708: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self[i, j] = values\n"],"name":"stderr"},{"output_type":"stream","text":["Num done: 0\n","Max accuracy on test set achieved: 79.799995%\n","Max suggested accuracy: 79.500000%\n","Current mean: 79.500000%\n","Current std: 0.000000\n","Num done: 1\n","Max accuracy on test set achieved: 80.400002%\n","Max suggested accuracy: 79.699997%\n","Current mean: 79.599998%\n","Current std: 0.099998\n","Num done: 2\n","Max accuracy on test set achieved: 80.299995%\n","Max suggested accuracy: 79.699997%\n","Current mean: 79.633331%\n","Current std: 0.094279\n","Num done: 3\n","Max accuracy on test set achieved: 80.699997%\n","Max suggested accuracy: 80.400002%\n","Current mean: 79.824997%\n","Current std: 0.341871\n","Num done: 4\n","Max accuracy on test set achieved: 81.099998%\n","Max suggested accuracy: 80.900002%\n","Current mean: 80.039993%\n","Current std: 0.527638\n","Num done: 5\n","Max accuracy on test set achieved: 80.699997%\n","Max suggested accuracy: 80.299995%\n","Current mean: 80.083328%\n","Current std: 0.491314\n","Num done: 6\n","Max accuracy on test set achieved: 80.400002%\n","Max suggested accuracy: 79.299995%\n","Current mean: 79.971428%\n","Current std: 0.531077\n","Num done: 7\n","Max accuracy on test set achieved: 80.199997%\n","Max suggested accuracy: 79.699997%\n","Current mean: 79.937500%\n","Current std: 0.504822\n","Num done: 8\n","Max accuracy on test set achieved: 80.800003%\n","Max suggested accuracy: 79.599998%\n","Current mean: 79.899994%\n","Current std: 0.487626\n","Num done: 9\n","Max accuracy on test set achieved: 80.400002%\n","Max suggested accuracy: 79.400002%\n","Current mean: 79.849998%\n","Current std: 0.486313\n"],"name":"stdout"}]},{"metadata":{"id":"UrK41BNzP9xC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"37c05d60-9e82-41a7-d461-831b21abc663","executionInfo":{"status":"ok","timestamp":1556208034416,"user_tz":-120,"elapsed":1410,"user":{"displayName":"Lucas Kania","photoUrl":"https://lh3.googleusercontent.com/-Atm5piH2aes/AAAAAAAAAAI/AAAAAAAAjl0/ClTgWH2onX0/s64/photo.jpg","userId":"02510383906759004102"}}},"cell_type":"code","source":["#Print average performance\n","print(np.mean(list_all_acc))\n","print(np.std(list_all_acc))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["79.85\n","0.48631346\n"],"name":"stdout"}]},{"metadata":{"id":"fYlg1IMxP9xF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}