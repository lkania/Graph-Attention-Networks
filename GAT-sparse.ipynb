{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAT-sparse.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-06T11:01:39.046641Z","start_time":"2019-06-06T11:01:31.913794Z"},"colab_type":"code","id":"lG_WpMphP9wp","scrolled":true,"colab":{}},"source":["import os,sys,inspect\n","import os\n","import joblib\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import scipy.sparse.linalg as la\n","import scipy.sparse as sp\n","import scipy\n","import time\n","import pickle\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","%matplotlib inline\n","\n","import scipy.io as sio\n","import process_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-06T11:22:55.552832Z","start_time":"2019-06-06T11:21:17.483904Z"},"colab_type":"code","executionInfo":{"status":"ok","timestamp":1559825232742,"user_tz":-120,"elapsed":4516604,"user":{"displayName":"Lucas Kania","photoUrl":"https://lh3.googleusercontent.com/-Atm5piH2aes/AAAAAAAAAAI/AAAAAAAAjl0/ClTgWH2onX0/s64/photo.jpg","userId":"02510383906759004102"}},"id":"Ks7k1tKGP9ww","outputId":"be290eee-17c7-4678-b357-f24ec84f67d7","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":1465}},"source":["class GAT:\n","    \n","    def frobenius_norm(self, tensor):\n","        square_tensor = tf.square(tensor)\n","        tensor_sum = tf.reduce_sum(square_tensor)\n","        frobenius_norm = tf.sqrt(tensor_sum)\n","        return frobenius_norm\n","    \n","    #def convert_coo_to_sparse_tensor(self, L):\n","    #    indices = np.column_stack((L.row, L.col))\n","   #     L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n","    #    L = tf.sparse_reorder(L)\n","    #    return L\n","    \n","    def coo_to_sparse_tensor(self,X):\n","        indices = np.mat([X.row, X.col]).transpose()\n","        return tf.SparseTensorValue(indices, X.data, X.shape)\n","      \n","    def concat(self,x,y):\n","        if x is None:\n","            return y\n","        if y is None:\n","            return x\n","        return tf.concat([x,y],axis=1)\n","    \n","    def __variable(self,shape,reg=True,name='weight'):\n","        v = tf.get_variable(name=name,shape=shape,dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n","        if reg:\n","            self.reg_variables.append(v)\n","        return v\n","    \n","    def __bias(self,shape,reg=True,name='bias'):\n","        v = tf.get_variable(name,shape=shape,dtype=tf.float32,initializer=tf.keras.initializers.Zeros())\n","        if reg:\n","            self.reg_variables.append(v)\n","        return v\n","        \n","    def __init__(self, A, X, Y,\n","                 num_hidden_feat,\n","                 K,\n","                 learning_rate,\n","                 gamma, # l2 regularization parameter\n","                 idx_gpu = '/GPU:0'):\n","        \n","        self.num_hidden_feat = num_hidden_feat\n","        self.learning_rate = learning_rate\n","        self.gamma=gamma\n","        self.num_layers=len(num_hidden_feat)\n","        \n","        self.K = K\n","        self.reg_variables = []\n","        \n","        with tf.Graph().as_default() as g:\n","                self.graph = g\n","                \n","                with tf.device(idx_gpu):\n","                        # dimensions\n","                        M = A.shape[0]\n","                  \n","                        #definition of constant matrices\n","                        self.A = self.coo_to_sparse_tensor(A.tocoo())\n","                        self.X = tf.constant(X, dtype=tf.float32) \n","                        self.Y = tf.constant(Y, dtype=tf.float32)\n","                        \n","                        #placeholder definition\n","                        self.idx_nodes = tf.placeholder(tf.int32)\n","                        self.keep_prob = tf.placeholder(tf.float32)\n","                        \n","                        #model definition\n","                        \n","                        #M = self.A.get_shape().as_list()[0]\n","                        M = tf.shape(self.A)[0]\n","                        A_ = tf.sparse.add(self.A,tf.sparse.eye(M,M,dtype=tf.float32))\n","                        \n","                        H_prev = self.X\n","                        \n","                        for layer in range(self.num_layers):\n","                          \n","                          with tf.variable_scope('layer_{0}'.format(layer)):\n","\n","                            Fout = self.num_hidden_feat[layer]\n","                            Fin = H_prev.get_shape().as_list()[1]\n","\n","                            H_prev = tf.nn.dropout(H_prev, self.keep_prob)\n","                            H = None\n","\n","                            for k in range(self.K[layer]):\n","                                \n","                                with tf.variable_scope('head_{0}'.format(k)):\n","                                \n","                                    W = self.__variable([Fin,Fout],name='W')\n","                                    A_1 = self.__variable([Fout,1],name='A_1')\n","                                    A_2 = self.__variable([Fout,1],name='A_2')\n","                                    b_1 = self.__bias([1],name='b_1')\n","                                    b_2 = self.__bias([1],name='b_2')\n","\n","                                    U = tf.matmul(H_prev,W) # M x Fout\n","\n","                                    gamma_1 = tf.matmul(U,A_1) + b_1 # M x 1\n","                                    gamma_2 = tf.matmul(U,A_2) + b_2 # M x 1\n","\n","                                    #phi = tf.add(\n","                                    #      tf.multiply(A_,gamma_1),\n","                                    #      tf.multiply(A_,tf.transpose(gamma_2))) # M x M\n","                                    \n","                                    phi = tf.sparse.add(\n","                                        A_.__mul__(gamma_1),\n","                                        A_.__mul__(tf.transpose(gamma_2))) # M x M\n","                                    \n","                                    phi = tf.SparseTensorValue(\n","                                        indices = phi.indices,\n","                                        values = tf.nn.leaky_relu(phi.values),\n","                                        dense_shape = [M,M]) # M x M\n","\n","                                    #phi = tf.nn.leaky_relu(phi,alpha=0.2)\n","\n","                                    #psi = tf.exp(phi) # M x M\n","                                    \n","                                    #psi = A_.__mul__(psi)\n","\n","                                    #theta = tf.sparse.reduce_sum(psi,axis=-1)\n","\n","                                    #Q = psi.__mul__(tf.reciprocal(theta))\n","                                    \n","                                    Q = tf.sparse.softmax(phi)\n","\n","                                    H_temp = tf.sparse.matmul(Q,U) # M x Fout                          \n","                                    H_temp = tf.nn.dropout(H_temp, self.keep_prob)\n","                                    \n","                                    H = self.concat(H,H_temp)\n","                            \n","                            H_prev = H\n","\n","                            if layer != self.num_layers-1 : # it is not the last layer\n","                                H_prev = tf.nn.elu(H_prev)\n","                        \n","                        self.logits = H_prev\n","                        \n","                        self.l_out = tf.gather(self.logits, self.idx_nodes)\n","                        self.c_Y = tf.gather(self.Y, self.idx_nodes)\n","                        \n","                        #loss function definition\n","                        \n","                        with tf.name_scope('loss'):\n","                            self.l2_reg = 0\n","                            for W in self.reg_variables:\n","                                self.l2_reg += tf.nn.l2_loss(W)\n","\n","                            self.data_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.l_out, labels=self.c_Y)) \n","                            self.loss = self.data_loss + self.gamma*self.l2_reg\n","\n","                        #solver definition\n","                        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n","                        self.opt_step = self.optimizer.minimize(self.loss)\n","                        \n","                        #predictions and accuracy extraction\n","                        self.c_predictions = tf.argmax(tf.nn.softmax(self.l_out), 1)\n","                        self.accuracy = tf.contrib.metrics.accuracy(self.c_predictions, tf.argmax(self.c_Y, 1))\n","                        \n","                        #gradients computation\n","                        self.trainable_variables = tf.trainable_variables()\n","                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n","                        \n","                        #for i,t in enumerate(tf.trainable_variables()):\n","                        #    print(str(t) + \" \" + str(self.var_grad[i]))\n","                        \n","                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n","                        \n","                        #session creation\n","                        config = tf.ConfigProto(allow_soft_placement = True)\n","                        config.gpu_options.allow_growth = True\n","                        self.session = tf.Session(config=config)\n","\n","                        #session initialization\n","                        init = tf.global_variables_initializer()\n","                        self.session.run(init)\n","                        \n","#learning parameters and path dataset\n","\n","num_total_iter_training = 3000\n","learning_rate = 0.005\n","val_test_interval = 1\n","num_hidden_feat = [8,7]\n","K = [8,1]\n","gamma = 0.0005\n","    \n","#dataset loadina\n","\n","A, X, Y, train_idx, val_idx, test_idx = process_data.load_data(\"cora\")\n","X = process_data.preprocess_features(X)\n","\n","# compute GCN adj matrix\n","A_tilde = sp.csr_matrix(A,dtype=np.float32)\n","A_tilde.setdiag(1)\n","D = A_tilde.sum(axis=1)\n","\n","D_rows, D_cols = D.nonzero()\n","D_vals = [D[i,j] for i, j in zip(D_rows, D_cols)]\n","D_vals = np.reciprocal(np.sqrt(np.asarray(D_vals)))\n","\n","D_inv_sqrt = sp.csr_matrix((D_vals, (range(len(D_vals)), range(len(D_vals)))))\n","\n","A_tilde = D_inv_sqrt.dot(A_tilde).dot(D_inv_sqrt)\n","A_tilde = A_tilde.tocsr()\n","A_tilde.eliminate_zeros()\n","\n","# Training\n","\n","num_exp = 10 #number of times training GCN over the given dataset\n","\n","list_all_acc = []\n","list_all_cost_val_avg  = []\n","list_all_data_cost_val_avg = []\n","list_all_acc_val_avg   = []\n","list_all_cost_test_avg = []\n","list_all_acc_test_avg  = []\n","\n","num_done = 0\n","for seed in range(num_exp):\n","    model = GAT(A=A_tilde, X=X, Y=Y, num_hidden_feat=num_hidden_feat, K=K, learning_rate=learning_rate, gamma=gamma)\n","\n","    cost_train_avg      = []\n","    grad_norm_train_avg = []\n","    acc_train_avg       = []\n","    cost_test_avg       = []\n","    grad_norm_test_avg  = []\n","    acc_test_avg        = []\n","    cost_val_avg        = []\n","    data_cost_val_avg   = []\n","    acc_val_avg         = []\n","    iter_test           = []\n","    list_training_time = list()\n","\n","    #Training code\n","    for i in range(num_total_iter_training):\n","        if (len(cost_train_avg) % val_test_interval) == 0:\n","\n","            #Validate the model\n","            \n","            feed_dict = {model.idx_nodes: val_idx, model.keep_prob:1.0}\n","            acc_val, cost_val, data_cost_val = \\\n","                model.session.run([model.accuracy, model.loss, model.data_loss], feed_dict)\n","            \n","            data_cost_val_avg.append(data_cost_val)\n","            cost_val_avg.append(cost_val)\n","            acc_val_avg.append(acc_val)\n","\n","            #Test the model\n","            \n","            feed_dict = {model.idx_nodes: test_idx, model.keep_prob:1.0}\n","            acc_test, cost_test = model.session.run([model.accuracy, model.loss], feed_dict)\n","            \n","            cost_test_avg.append(cost_test)\n","            acc_test_avg.append(acc_test)\n","            iter_test.append(len(cost_train_avg))\n","\n","        tic = time.time()\n","        feed_dict = {model.idx_nodes: train_idx, model.keep_prob: 0.5}\n","        \n","        _, current_training_loss, norm_grad, current_acc_training = \\\n","            model.session.run([model.opt_step, model.loss, model.norm_grad, model.accuracy], feed_dict) \n","\n","        training_time = time.time() - tic   \n","\n","        cost_train_avg.append(current_training_loss)\n","        grad_norm_train_avg.append(norm_grad)\n","        acc_train_avg.append(current_acc_training)\n","\n","    #Compute and print statistics of the last realized experiment\n","    list_all_acc.append(100*(np.asarray(acc_test_avg)[np.asarray(data_cost_val_avg)==np.min(data_cost_val_avg)]))\n","    list_all_cost_val_avg.append(cost_val_avg)\n","    list_all_data_cost_val_avg.append(data_cost_val_avg)\n","    list_all_acc_val_avg.append(acc_val_avg)\n","    list_all_cost_test_avg.append(cost_test_avg)\n","    list_all_acc_test_avg.append(acc_test_avg)\n","\n","    print('Num done: %d' % num_done)\n","    print('Max accuracy on test set achieved: %f%%' % np.max(np.asarray(acc_test_avg)*100))\n","    print('Max suggested accuracy: %f%%' % (100*(np.asarray(acc_test_avg)[np.asarray(data_cost_val_avg)==np.min(data_cost_val_avg)]),))\n","    print('Current mean: %f%%' % np.mean(list_all_acc))\n","    print('Current std: %f' % np.std(list_all_acc))\n","\n","    num_done += 1"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(2708, 2708)\n","(2708, 1433)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:112: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-90ee43e7cc9e>:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-3-90ee43e7cc9e>:149: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_grad.py:281: calling sparse_reduce_sum (from tensorflow.python.ops.sparse_ops) with keep_dims is deprecated and will be removed in a future version.\n","Instructions for updating:\n","keep_dims is deprecated, use keepdims instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_grad.py:216: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Num done: 0\n","Max accuracy on test set achieved: 82.800003%\n","Max suggested accuracy: 81.599998%\n","Current mean: 81.599998%\n","Current std: 0.000000\n","Num done: 1\n","Max accuracy on test set achieved: 82.599998%\n","Max suggested accuracy: 81.500000%\n","Current mean: 81.550003%\n","Current std: 0.049999\n","Num done: 2\n","Max accuracy on test set achieved: 82.700005%\n","Max suggested accuracy: 81.699997%\n","Current mean: 81.599998%\n","Current std: 0.081648\n","Num done: 3\n","Max accuracy on test set achieved: 82.599998%\n","Max suggested accuracy: 81.800003%\n","Current mean: 81.650002%\n","Current std: 0.111804\n","Num done: 4\n","Max accuracy on test set achieved: 82.500000%\n","Max suggested accuracy: 82.000000%\n","Current mean: 81.720001%\n","Current std: 0.172047\n","Num done: 5\n","Max accuracy on test set achieved: 82.700005%\n","Max suggested accuracy: 81.900002%\n","Current mean: 81.750000%\n","Current std: 0.170783\n","Num done: 6\n","Max accuracy on test set achieved: 83.099998%\n","Max suggested accuracy: 82.599998%\n","Current mean: 81.871422%\n","Current std: 0.336852\n","Num done: 7\n","Max accuracy on test set achieved: 82.500000%\n","Max suggested accuracy: 81.199997%\n","Current mean: 81.787498%\n","Current std: 0.385479\n","Num done: 8\n","Max accuracy on test set achieved: 82.599998%\n","Max suggested accuracy: 82.099998%\n","Current mean: 81.822220%\n","Current std: 0.376469\n","Num done: 9\n","Max accuracy on test set achieved: 82.599998%\n","Max suggested accuracy: 81.500000%\n","Current mean: 81.789993%\n","Current std: 0.370000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1559826397579,"user_tz":-120,"elapsed":637,"user":{"displayName":"Lucas Kania","photoUrl":"https://lh3.googleusercontent.com/-Atm5piH2aes/AAAAAAAAAAI/AAAAAAAAjl0/ClTgWH2onX0/s64/photo.jpg","userId":"02510383906759004102"}},"id":"UrK41BNzP9xC","outputId":"661ebc61-367f-4652-dded-5da3f4245d97","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Print average performance\n","print(np.mean(list_all_acc))\n","print(np.std(list_all_acc))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["81.78999\n","0.3700002\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fYlg1IMxP9xF","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}